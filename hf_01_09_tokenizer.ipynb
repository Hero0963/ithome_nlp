{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hero0963/ithome_nlp/blob/main/hf_01_09_tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a52604a3",
      "metadata": {
        "id": "a52604a3"
      },
      "outputs": [],
      "source": [
        "# ref = https://ithelp.ithome.com.tw/articles/10298516"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df5997d3",
      "metadata": {
        "id": "df5997d3",
        "outputId": "6b274c45-276e-4cf1-e76e-f5ba60378d0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['W', 'h', 'e', 'n', ' ', 'i', 'n', ' ', 'e', 't', 'e', 'r', 'n', 'a', 'l', ' ', 'l', 'i', 'n', 'e', 's', ' ', 't', 'o', ' ', 't', 'i', 'm', 'e', ' ', 't', 'h', 'o', 'u', ' ', 'g', 'r', 'o', 'w', 'e', 's', 't', ':', ' ', 'S', 'o', ' ', 'l', 'o', 'n', 'g', ' ', 'a', 's', ' ', 'm', 'e', 'n', ' ', 'c', 'a', 'n', ' ', 'b', 'r', 'e', 'a', 't', 'h', 'e', ' ', 'o', 'r', ' ', 'e', 'y', 'e', 's', ' ', 'c', 'a', 'n', ' ', 's', 'e', 'e', ',', ' ', 'S', 'o', ' ', 'l', 'o', 'n', 'g', ' ', 'l', 'i', 'v', 'e', 's', ' ', 't', 'h', 'i', 's', ' ', 'a', 'n', 'd', ' ', 't', 'h', 'i', 's', ' ', 'g', 'i', 'v', 'e', 's', ' ', 'l', 'i', 'f', 'e', ' ', 't', 'o', ' ', 't', 'h', 'e', 'e', '.']\n"
          ]
        }
      ],
      "source": [
        "# https://www.harpersbazaar.com/tw/culture/exhibition/news/a497/literature-shakespearessonnets/\n",
        "\n",
        "string = \"When in eternal lines to time thou growest: So long as men can breathe or eyes can see, So long lives this and this gives life to thee.\"\n",
        "tokenized_str = list(string)\n",
        "print(tokenized_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a927b889",
      "metadata": {
        "id": "a927b889",
        "outputId": "b6e6398f-789f-4ef2-a0c9-f8a92596b13f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{' ': 0, ',': 1, '.': 2, ':': 3, 'S': 4, 'W': 5, 'a': 6, 'b': 7, 'c': 8, 'd': 9, 'e': 10, 'f': 11, 'g': 12, 'h': 13, 'i': 14, 'l': 15, 'm': 16, 'n': 17, 'o': 18, 'r': 19, 's': 20, 't': 21, 'u': 22, 'v': 23, 'w': 24, 'y': 25}\n"
          ]
        }
      ],
      "source": [
        "token2idx = {ch: idx for idx, ch in enumerate(sorted(set(tokenized_str)))}\n",
        "print(token2idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1fa88cb",
      "metadata": {
        "id": "a1fa88cb",
        "outputId": "c57b7f6c-f182-4fd3-9bee-87b0cd5e7ae5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[5, 13, 10, 17, 0, 14, 17, 0, 10, 21, 10, 19, 17, 6, 15, 0, 15, 14, 17, 10, 20, 0, 21, 18, 0, 21, 14, 16, 10, 0, 21, 13, 18, 22, 0, 12, 19, 18, 24, 10, 20, 21, 3, 0, 4, 18, 0, 15, 18, 17, 12, 0, 6, 20, 0, 16, 10, 17, 0, 8, 6, 17, 0, 7, 19, 10, 6, 21, 13, 10, 0, 18, 19, 0, 10, 25, 10, 20, 0, 8, 6, 17, 0, 20, 10, 10, 1, 0, 4, 18, 0, 15, 18, 17, 12, 0, 15, 14, 23, 10, 20, 0, 21, 13, 14, 20, 0, 6, 17, 9, 0, 21, 13, 14, 20, 0, 12, 14, 23, 10, 20, 0, 15, 14, 11, 10, 0, 21, 18, 0, 21, 13, 10, 10, 2]\n"
          ]
        }
      ],
      "source": [
        "input_ids = [token2idx[token] for token in tokenized_str]\n",
        "print(input_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3402447",
      "metadata": {
        "id": "b3402447",
        "outputId": "952e089f-e486-4dc5-db9a-ade8dd7bce40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['When', 'in', 'eternal', 'lines', 'to', 'time', 'thou', 'growest:', 'So', 'long', 'as', 'men', 'can', 'breathe', 'or', 'eyes', 'can', 'see,', 'So', 'long', 'lives', 'this', 'and', 'this', 'gives', 'life', 'to', 'thee.']\n"
          ]
        }
      ],
      "source": [
        "string = \"When in eternal lines to time thou growest: So long as men can breathe or eyes can see, So long lives this and this gives life to thee.\"\n",
        "tokenized_str = string.split()\n",
        "print(tokenized_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d886d3b",
      "metadata": {
        "id": "0d886d3b",
        "outputId": "bf5d5c9e-d534-4b43-da50-874b23a57bb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'So': 0, 'When': 1, 'and': 2, 'as': 3, 'breathe': 4, 'can': 5, 'eternal': 6, 'eyes': 7, 'gives': 8, 'growest:': 9, 'in': 10, 'life': 11, 'lines': 12, 'lives': 13, 'long': 14, 'men': 15, 'or': 16, 'see,': 17, 'thee.': 18, 'this': 19, 'thou': 20, 'time': 21, 'to': 22}\n"
          ]
        }
      ],
      "source": [
        "token_word2idx = {ch: idx for idx, ch in enumerate(sorted(set(tokenized_str)))}\n",
        "print(token_word2idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77c23555",
      "metadata": {
        "id": "77c23555",
        "outputId": "8b7bc409-a061-441b-a2b5-b51d1fd40535"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 10, 6, 12, 22, 21, 20, 9, 0, 14, 3, 15, 5, 4, 16, 7, 5, 17, 0, 14, 13, 19, 2, 19, 8, 11, 22, 18]\n"
          ]
        }
      ],
      "source": [
        "input_ids = [token_word2idx[token] for token in tokenized_str]\n",
        "print(input_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b79263c2",
      "metadata": {
        "id": "b79263c2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}